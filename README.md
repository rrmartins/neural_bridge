# ğŸ§  NeuralBridge - Proxy LLM Inteligente

> **Proxy avanÃ§ado entre aplicaÃ§Ãµes e provedores de LLM com configuraÃ§Ã£o via .env**

NeuralBridge Ã© um proxy inteligente que atua como intermediÃ¡rio entre suas aplicaÃ§Ãµes e diferentes provedores de LLM (OpenAI, Ollama), implementando cache inteligente, RAG (Retrieval-Augmented Generation), guardrails de seguranÃ§a e fallback automÃ¡tico. Configure facilmente via variÃ¡veis de ambiente qual provedor usar como padrÃ£o.

## âœ¨ CaracterÃ­sticas Principais

- âš™ï¸ **ConfiguraÃ§Ã£o .env** - Defina o provedor padrÃ£o (OpenAI/Ollama) via variÃ¡veis de ambiente
- ğŸ”„ **Pipeline Inteligente** - Cache â†’ RAG â†’ LLM â†’ API B (fallback)
- ğŸ¤– **Multi-Provider** - OpenAI e Ollama com modelos configurÃ¡veis
- ğŸ§  **RAG Integrado** - Busca semÃ¢ntica em base de conhecimento
- ğŸ›¡ï¸ **Guardrails** - ValidaÃ§Ã£o de seguranÃ§a e qualidade
- ğŸ“Š **Observabilidade** - MÃ©tricas em tempo real e telemetria
- ğŸ”„ **Background Jobs** - Processamento assÃ­ncrono de embeddings e treinamento
- ğŸ’¾ **Cache HÃ­brido** - MemÃ³ria + PostgreSQL persistente
- ğŸŒ **WebSockets** - Streaming de respostas em tempo real

## ğŸš€ InÃ­cio RÃ¡pido

### 1. InstalaÃ§Ã£o

```bash
# Clone o repositÃ³rio
git clone <repository-url>
cd neural_bridge

# Instale dependÃªncias
mix setup

# Configure o banco de dados
mix ecto.create
mix ecto.migrate
```

### 2. ConfiguraÃ§Ã£o (.env)

**OpÃ§Ã£o 1 - OpenAI:**
```bash
# Copie o arquivo de exemplo
cp .env.example .env

# Configure para OpenAI
echo "LLM_PROVIDER=openai" > .env
echo "OPENAI_API_KEY=sk-your-key-here" >> .env
echo "OPENAI_DEFAULT_MODEL=gpt-4" >> .env
```

**OpÃ§Ã£o 2 - Ollama:**
```bash
# Instale Ollama
curl -fsSL https://ollama.com/install.sh | sh

# Baixe modelos
ollama pull llama2

# Configure para Ollama
echo "LLM_PROVIDER=ollama" > .env
echo "OLLAMA_DEFAULT_MODEL=llama2" >> .env
```

### 3. ExecuÃ§Ã£o

```bash
# Inicie o servidor
mix phx.server

# Teste a configuraÃ§Ã£o
curl http://localhost:4000/api/proxy/health
```

## ğŸ”§ ConfiguraÃ§Ã£o via .env

O NeuralBridge usa variÃ¡veis de ambiente para determinar automaticamente qual provedor LLM usar:

### Arquivo .env exemplo:

```bash
# Provedor principal (openai ou ollama)
LLM_PROVIDER=ollama

# ConfiguraÃ§Ã£o OpenAI
OPENAI_API_KEY=sk-your-key-here
OPENAI_DEFAULT_MODEL=gpt-4
OPENAI_DEFAULT_TEMPERATURE=0.7

# ConfiguraÃ§Ã£o Ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=llama2

# Cache e outras configuraÃ§Ãµes
CACHE_ENABLED=true
CACHE_TTL_SECONDS=3600
```

### Modelos Suportados:

**OpenAI:** `gpt-4`, `gpt-4-turbo`, `gpt-3.5-turbo`

**Ollama:** `llama2`, `codellama`, `mistral`, `neural-chat`, modelos personalizados (`llama2:7b-chat`)

## ğŸ“¡ API REST

### Endpoint Principal

```bash
# Usa configuraÃ§Ã£o padrÃ£o do .env
curl -X POST "http://localhost:4000/api/proxy/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Explique machine learning",
    "session_id": "minha_sessao_123"
  }'

# Override do provedor na requisiÃ§Ã£o
curl -X POST "http://localhost:4000/api/proxy/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Explique machine learning",
    "session_id": "minha_sessao_123",
    "provider": "openai",
    "model": "gpt-4"
  }'
```

### Endpoints de Monitoramento

```bash
# Verificar saÃºde
curl http://localhost:4000/api/proxy/health

# Obter estatÃ­sticas
curl http://localhost:4000/api/proxy/stats
```

## ğŸ”„ Pipeline de Processamento

```
1. ğŸ’¾ Cache Check â†’ Resposta instantÃ¢nea se encontrada
2. ğŸ§  RAG Retrieval â†’ Busca contexto na base de conhecimento
3. ğŸ¤– LLM Generation â†’ OpenAI ou Ollama
4. ğŸ›¡ï¸ Guardrails â†’ ValidaÃ§Ã£o de seguranÃ§a
5. ğŸ”„ API B Fallback â†’ Fallback automÃ¡tico se necessÃ¡rio
```

## ğŸ“Š Observabilidade

- **PromEx Integration** - MÃ©tricas Prometheus
- **Real-time Stats** - Interface web com mÃ©tricas ao vivo
- **Health Checks** - Monitoramento de todos os componentes
- **Telemetry Events** - Rastreamento detalhado de operaÃ§Ãµes

## ğŸ”§ Arquitetura

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Client App â”‚â”€â”€â”€â”€â”‚ NeuralBridge â”‚â”€â”€â”€â”€â”‚   LLM APIs  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚               â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚  Cache  â”‚    â”‚   RAG    â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚              â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ Cachex  â”‚    â”‚PostgreSQLâ”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Componentes Principais:

- **ConversationServer** - GenServer para gestÃ£o de sessÃµes
- **Cache** - Sistema hÃ­brido Cachex + PostgreSQL
- **RAG** - Retrieval-Augmented Generation com embeddings
- **LLM** - Cliente unificado OpenAI/Ollama
- **Guardrails** - ValidaÃ§Ã£o de seguranÃ§a e qualidade
- **Background Jobs** - Oban para processamento assÃ­ncrono

## ğŸ“š DocumentaÃ§Ã£o

- **[Guia de ConfiguraÃ§Ã£o](docs/CONFIG_GUIDE.md)** - Setup detalhado OpenAI/Ollama
- **[API Interactions](docs/API_INTERACTIONS.md)** - DocumentaÃ§Ã£o completa da API
- **[WebSocket Guide](docs/WEBSOCKET_INTERACTIONS.md)** - Streaming em tempo real
- **[Background Jobs](docs/BACKGROUND_JOBS.md)** - Sistema de jobs assÃ­ncronos

## ğŸ› ï¸ Desenvolvimento

### Estrutura de Arquivos

```
lib/
â”œâ”€â”€ neural_bridge/
â”‚   â”œâ”€â”€ application.ex          # Supervisor principal
â”‚   â”œâ”€â”€ conversation_server.ex  # GestÃ£o de sessÃµes
â”‚   â”œâ”€â”€ cache.ex               # Sistema de cache
â”‚   â”œâ”€â”€ rag.ex                 # Retrieval-Augmented Generation
â”‚   â”œâ”€â”€ llm.ex                 # Cliente LLM unificado
â”‚   â”œâ”€â”€ guardrails.ex          # ValidaÃ§Ãµes de seguranÃ§a
â”‚   â””â”€â”€ workers/               # Background jobs
â””â”€â”€ neural_bridge_web/
    â”œâ”€â”€ channels/              # WebSocket channels
    â”œâ”€â”€ controllers/           # REST controllers
    â””â”€â”€ router.ex             # Roteamento
```

### Background Jobs

- **EmbedJob** - GeraÃ§Ã£o de embeddings para RAG
- **TrainJob** - Treinamento de modelos (fine-tuning/distillation)
- **CacheCleanupWorker** - Limpeza automÃ¡tica de cache
- **TrainingDatasetWorker** - AnÃ¡lise e trigger de treinamentos

### Executar Testes

```bash
mix test
```

### Executar em Desenvolvimento

```bash
# Com live reloading
mix phx.server

# No console interativo
iex -S mix phx.server
```

## ğŸ“ˆ Monitoramento em ProduÃ§Ã£o

### MÃ©tricas DisponÃ­veis

- Cache hit/miss rates
- Response times por provider
- Confidence scores
- API B fallback rates
- Training job performance
- System resources

### Health Checks

- Database connectivity
- Cache system status
- LLM provider availability
- API B connectivity

## ğŸ” SeguranÃ§a

- **Guardrails** automÃ¡ticos para validaÃ§Ã£o de conteÃºdo
- **PII Detection** e remoÃ§Ã£o
- **Rate limiting** por sessÃ£o
- **Input sanitization**
- **Secure session management**

## ğŸš€ Deploy em ProduÃ§Ã£o

```bash
# Build release
MIX_ENV=prod mix release

# Execute
_build/prod/rel/neural_bridge/bin/neural_bridge start
```

### VariÃ¡veis de Ambiente

```bash
# LLM Providers
OPENAI_API_KEY=sk-...
OLLAMA_BASE_URL=http://localhost:11434

# Database
DATABASE_URL=ecto://user:pass@localhost/neural_bridge_prod

# Cache
CACHE_TTL_SECONDS=3600
CACHE_MAX_SIZE=10000

# Security
SECRET_KEY_BASE=your-secret-key
```

## ğŸ¤ Contribuindo

1. Fork o projeto
2. Crie uma feature branch (`git checkout -b feature/nova-funcionalidade`)
3. Commit suas mudanÃ§as (`git commit -am 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/nova-funcionalidade`)
5. Abra um Pull Request

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a MIT License - veja o arquivo [LICENSE](LICENSE) para detalhes.

## ğŸ”— Links Ãšteis

- **Phoenix Framework**: https://www.phoenixframework.org/
- **OpenAI API**: https://platform.openai.com/docs
- **Ollama**: https://ollama.com/
- **Oban**: https://github.com/sorentwo/oban
- **PromEx**: https://github.com/akoutmos/prom_ex

---

**ğŸ‰ Comece agora: `mix phx.server` e acesse `http://localhost:4000`**