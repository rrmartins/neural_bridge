# üîß NeuralBridge - Guia de Configura√ß√£o (.env)

## üéØ Vis√£o Geral

Este guia explica como configurar o NeuralBridge usando vari√°veis de ambiente (`.env`) para determinar automaticamente qual provedor LLM usar (OpenAI ou Ollama) e personalizar todas as configura√ß√µes do sistema.

---

## üìÅ Configura√ß√£o B√°sica

### 1. Copie o arquivo de exemplo

```bash
cp .env.example .env
```

### 2. Configure seu provedor principal

O NeuralBridge automaticamente usa o provedor definido em `LLM_PROVIDER`:

```bash
# Para usar OpenAI como padr√£o
LLM_PROVIDER=openai

# Para usar Ollama como padr√£o
LLM_PROVIDER=ollama
```

---

## ü§ñ Configura√ß√£o OpenAI

Para usar OpenAI como provedor principal, configure:

```bash
# Provedor principal
LLM_PROVIDER=openai

# Chave da API (obtenha em: https://platform.openai.com/api-keys)
OPENAI_API_KEY=sk-your-openai-api-key-here

# Configura√ß√µes opcionais
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_DEFAULT_MODEL=gpt-4
OPENAI_DEFAULT_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=2048
```

### Modelos OpenAI Dispon√≠veis:
- `gpt-4` - Mais inteligente
- `gpt-4-turbo` - Velocidade otimizada
- `gpt-3.5-turbo` - Custo-benef√≠cio

### Exemplo completo para OpenAI:

```bash
# .env
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-proj-abc123...
OPENAI_DEFAULT_MODEL=gpt-4
OPENAI_DEFAULT_TEMPERATURE=0.7
```

---

## ü¶ô Configura√ß√£o Ollama

Para usar Ollama como provedor principal:

### 1. Instale o Ollama

```bash
# macOS/Linux
curl -fsSL https://ollama.com/install.sh | sh

# Windows: baixe de https://ollama.com/download
```

### 2. Baixe modelos

```bash
ollama pull llama2
ollama pull codellama
ollama pull mistral
ollama pull neural-chat
```

### 3. Configure o .env

```bash
# Provedor principal
LLM_PROVIDER=ollama

# URL do Ollama (padr√£o: localhost:11434)
OLLAMA_BASE_URL=http://localhost:11434

# Modelo padr√£o
OLLAMA_DEFAULT_MODEL=llama2
```

### Modelos Ollama Suportados:
- `llama2` - Modelo geral robusto
- `codellama` - Especializado em c√≥digo
- `mistral` - Modelo eficiente europeu
- `neural-chat` - Otimizado para conversas
- Modelos personalizados: `llama2:7b-chat`, `llama2:13b-chat`, etc.

### Exemplo completo para Ollama:

```bash
# .env
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=codellama
```

---

## üîÑ Mudan√ßa Din√¢mica de Provedor

### Via Vari√°vel de Ambiente

```bash
# Usar OpenAI
export LLM_PROVIDER=openai
mix phx.server

# Usar Ollama
export LLM_PROVIDER=ollama
mix phx.server
```

### Via API (override por requisi√ß√£o)

Mesmo com um provedor padr√£o configurado, voc√™ pode especificar outro na requisi√ß√£o:

```bash
# Servidor configurado para Ollama, mas usando OpenAI nesta chamada
curl -X POST "http://localhost:4000/api/proxy/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Explique machine learning",
    "session_id": "test_123",
    "provider": "openai",
    "model": "gpt-4"
  }'
```

---

## üìã Arquivo .env Completo

Aqui est√° um exemplo completo de `.env` com todas as op√ß√µes:

```bash
# ================================
# NeuralBridge - Configura√ß√£o LLM
# ================================

# =====================
# Provedor Principal
# =====================
LLM_PROVIDER=ollama

# =====================
# Configura√ß√£o OpenAI
# =====================
OPENAI_API_KEY=sk-your-openai-api-key-here
OPENAI_BASE_URL=https://api.openai.com/v1
OPENAI_DEFAULT_MODEL=gpt-4
OPENAI_DEFAULT_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=2048

# =====================
# Configura√ß√£o Ollama
# =====================
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=llama2

# =====================
# API B (Fallback)
# =====================
API_B_ENABLED=true
API_B_URL=https://api.exemplo.com/v1/chat
API_B_TOKEN=seu-token-api-b-aqui
API_B_TIMEOUT_MS=30000

# =====================
# Cache Configuration
# =====================
CACHE_ENABLED=true
CACHE_TTL_SECONDS=3600
CACHE_MAX_SIZE=10000
CACHE_CLEANUP_INTERVAL_MINUTES=60

# =====================
# RAG Configuration
# =====================
RAG_ENABLED=true
RAG_SIMILARITY_THRESHOLD=0.7
RAG_MAX_CHUNKS=5
RAG_CHUNK_SIZE=1000
RAG_CHUNK_OVERLAP=200

# =====================
# Guardrails
# =====================
GUARDRAILS_ENABLED=true
GUARDRAILS_MIN_CONFIDENCE=0.7
GUARDRAILS_PII_DETECTION=true
GUARDRAILS_TOXICITY_CHECK=true

# =====================
# Background Jobs
# =====================
OBAN_ENABLED=true
EMBEDDING_QUEUE_LIMIT=5
TRAINING_QUEUE_LIMIT=3
DEFAULT_QUEUE_LIMIT=10

# =====================
# Database
# =====================
DATABASE_URL=ecto://postgres:postgres@localhost/neural_bridge_dev

# =====================
# Server Configuration
# =====================
PORT=4000
SECRET_KEY_BASE=your-secret-key-base-here
PHX_HOST=localhost

# =====================
# Monitoring
# =====================
PROM_EX_ENABLED=false
TELEMETRY_ENABLED=true
HEALTH_CHECK_INTERVAL_SECONDS=30

# =====================
# Development/Debug
# =====================
LOG_LEVEL=info
DEBUG_MODE=false
CODE_RELOADER=true
```

---

## üß™ Testando a Configura√ß√£o

### 1. Verificar configura√ß√£o atual

```bash
# Testar sa√∫de do sistema
curl http://localhost:4000/api/proxy/health

# Ver estat√≠sticas
curl http://localhost:4000/api/proxy/stats
```

### 2. Teste com provedor padr√£o

```bash
# Usar configura√ß√£o padr√£o do .env
curl -X POST "http://localhost:4000/api/proxy/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Ol√°, como voc√™ est√°?",
    "session_id": "test_session_123"
  }'
```

### 3. Teste com provedor espec√≠fico

```bash
# For√ßar uso de OpenAI (mesmo se Ollama for padr√£o)
curl -X POST "http://localhost:4000/api/proxy/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "Qual √© a capital do Brasil?",
    "session_id": "test_session_456",
    "provider": "openai",
    "model": "gpt-3.5-turbo"
  }'
```

---

## üîÑ Cen√°rios de Uso Comuns

### Desenvolvimento com Ollama

```bash
# .env para desenvolvimento local
LLM_PROVIDER=ollama
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=llama2
CACHE_ENABLED=true
DEBUG_MODE=true
LOG_LEVEL=debug
```

### Produ√ß√£o com OpenAI

```bash
# .env para produ√ß√£o
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-prod-key-here
OPENAI_DEFAULT_MODEL=gpt-4
OPENAI_MAX_TOKENS=4096
CACHE_ENABLED=true
CACHE_TTL_SECONDS=7200
GUARDRAILS_ENABLED=true
PROM_EX_ENABLED=true
```

### Ambiente H√≠brido

```bash
# OpenAI como padr√£o, Ollama dispon√≠vel para override
LLM_PROVIDER=openai
OPENAI_API_KEY=sk-key-here
OPENAI_DEFAULT_MODEL=gpt-4

# Ollama tamb√©m configurado
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_DEFAULT_MODEL=codellama

# Fallback robusto
API_B_ENABLED=true
API_B_URL=https://backup-api.com/v1/chat
API_B_TOKEN=backup-token
```

---

## üöÄ Inicializa√ß√£o R√°pida

### Para OpenAI:

```bash
# 1. Configure
echo "LLM_PROVIDER=openai" > .env
echo "OPENAI_API_KEY=sk-your-key-here" >> .env

# 2. Inicie
mix phx.server

# 3. Teste
curl -X POST "http://localhost:4000/api/proxy/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "Ol√°!", "session_id": "test"}'
```

### Para Ollama:

```bash
# 1. Instale Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 2. Baixe modelo
ollama pull llama2

# 3. Configure
echo "LLM_PROVIDER=ollama" > .env
echo "OLLAMA_DEFAULT_MODEL=llama2" >> .env

# 4. Inicie
mix phx.server

# 5. Teste
curl -X POST "http://localhost:4000/api/proxy/query" \
  -H "Content-Type: application/json" \
  -d '{"query": "Ol√°!", "session_id": "test"}'
```

---

## üîß Solu√ß√£o de Problemas

### OpenAI

| Problema | Solu√ß√£o |
|----------|---------|
| `missing_api_key` | Defina `OPENAI_API_KEY` no `.env` |
| `Authentication failed` | Verifique se a chave da API est√° correta |
| `Model not found` | Use um modelo v√°lido: `gpt-4`, `gpt-3.5-turbo` |
| `Rate limit exceeded` | Aguarde ou mude para Ollama temporariamente |

### Ollama

| Problema | Solu√ß√£o |
|----------|---------|
| `Connection refused` | Verifique se Ollama est√° rodando: `ollama serve` |
| `Model not found` | Baixe o modelo: `ollama pull llama2` |
| `Out of memory` | Use modelo menor: `llama2:7b` em vez de `llama2:70b` |

### Configura√ß√£o

| Problema | Solu√ß√£o |
|----------|---------|
| Configura√ß√£o n√£o carrega | Reinicie o servidor: `mix phx.server` |
| Provedor errado | Verifique `LLM_PROVIDER` no `.env` |
| Modelo n√£o encontrado | Verifique `*_DEFAULT_MODEL` no `.env` |

---

## üìö Pr√≥ximos Passos

1. **Configure seu `.env`** com o provedor preferido
2. **Teste ambos os provedores** para comparar performance
3. **Ajuste par√¢metros** como temperatura e max_tokens
4. **Configure cache e RAG** para melhor performance
5. **Monitore m√©tricas** via endpoints de sa√∫de

---

**üéâ O NeuralBridge est√° configurado! Use `mix phx.server` e comece a fazer requisi√ß√µes.**