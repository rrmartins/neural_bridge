# ğŸš€ NeuralBridge - Setup RÃ¡pido

## âš¡ ConfiguraÃ§Ã£o em 3 Passos

### 1ï¸âƒ£ Clone e Configure

```bash
# Clone o projeto
git clone <repository-url>
cd neural_bridge

# Instale dependÃªncias
mix setup
mix ecto.create
mix ecto.migrate
```

### 2ï¸âƒ£ Configure seu Provedor LLM

**OpÃ§Ã£o A - OpenAI:**
```bash
echo "LLM_PROVIDER=openai" > .env
echo "OPENAI_API_KEY=sk-your-key-here" >> .env
echo "OPENAI_DEFAULT_MODEL=gpt-4" >> .env
```

**OpÃ§Ã£o B - Ollama:**
```bash
# Instale Ollama primeiro
curl -fsSL https://ollama.com/install.sh | sh
ollama pull llama2

# Configure .env
echo "LLM_PROVIDER=ollama" > .env
echo "OLLAMA_DEFAULT_MODEL=llama2" >> .env
```

### 3ï¸âƒ£ Execute e Teste

```bash
# Inicie o servidor
mix phx.server

# Teste a configuraÃ§Ã£o
curl http://localhost:4000/api/proxy/health

# FaÃ§a uma pergunta
curl -X POST "http://localhost:4000/api/proxy/query" \
  -H "Content-Type: application/json" \
  -d '{
    "query": "OlÃ¡, como vocÃª estÃ¡?",
    "session_id": "test_123"
  }'
```

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

Para configuraÃ§Ã£o completa, veja:
- [CONFIG_GUIDE.md](docs/CONFIG_GUIDE.md) - Guia detalhado de configuraÃ§Ã£o
- [.env.example](.env.example) - Todas as opÃ§Ãµes disponÃ­veis

## ğŸ¤– MudanÃ§a de Provedor

```bash
# Para trocar de OpenAI para Ollama
sed -i 's/LLM_PROVIDER=openai/LLM_PROVIDER=ollama/' .env

# Reinicie o servidor
mix phx.server
```

## âœ… VerificaÃ§Ã£o

- âœ… Servidor rodando em `http://localhost:4000`
- âœ… Health check funcionando
- âœ… API respondendo com provedor configurado
- âœ… Cache, RAG e Guardrails ativos

**ğŸ‰ Pronto! Seu proxy LLM estÃ¡ funcionando.**