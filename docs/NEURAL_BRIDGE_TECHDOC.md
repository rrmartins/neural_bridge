# NeuralBridge - DocumentaÃ§Ã£o TÃ©cnica

## ğŸ“‹ VisÃ£o Geral

**NeuralBridge** Ã© um proxy LLM inteligente construÃ­do em Elixir/Phoenix que atua como uma camada intermediÃ¡ria entre aplicaÃ§Ãµes cliente e provedores de LLM externos. O sistema implementa uma estratÃ©gia de fallback em cascata: Cache â†’ RAG â†’ LLM Local/OpenAI â†’ API B externa.

## ğŸ—ï¸ Arquitetura

### Componentes Principais

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Cliente A     â”‚â”€â”€â”€â–¶â”‚  NeuralBridge   â”‚â”€â”€â”€â–¶â”‚    API B        â”‚
â”‚   (Projeto A)   â”‚    â”‚     Proxy       â”‚    â”‚   (Fallback)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                              â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Cache + RAG +  â”‚
                    â”‚  LLM (OpenAI/   â”‚
                    â”‚     Ollama)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pipeline de DecisÃ£o

1. **Cache Check** - Verifica cache em memÃ³ria (Cachex) e persistente (PostgreSQL)
2. **RAG Retrieval** - Busca contexto semÃ¢ntico em pgvector
3. **LLM Generation** - Gera resposta com OpenAI ou Ollama
4. **API B Fallback** - Usa API externa se confianÃ§a < threshold
5. **Logging & Training** - Registra todos os pares (query â†’ response)

## ğŸ”§ Stack TecnolÃ³gico

- **Phoenix Framework** - API REST + WebSockets
- **Oban** - Processamento de jobs em background
- **Cachex + ETS** - Cache hÃ­brido (memÃ³ria + PostgreSQL)
- **PostgreSQL + pgvector** - Banco principal + busca semÃ¢ntica
- **PromEx** - Observabilidade e mÃ©tricas
- **Req/Finch** - Cliente HTTP para APIs externas
- **Broadway** - IngestÃ£o de dados em larga escala

## ğŸ“Š Esquema do Banco de Dados

### Tabelas Principais

```sql
-- Conversas e sessÃµes
conversations (id, session_id, user_id, metadata, last_activity_at)
messages (id, conversation_id, role, content, source, confidence_score)

-- Base de conhecimento (RAG)
knowledge_chunks (id, source_document, content, embedding, metadata)

-- Logs e treinamento
query_logs (id, query, response, source, confidence_score, api_b_called)
training_jobs (id, status, job_type, progress_percentage, results)

-- Cache persistente
cache_entries (id, query_hash, query, response, hit_count, expires_at)
```

## ğŸš€ APIs e Endpoints

### REST API

```http
POST   /api/proxy/query           # Processa query principal
GET    /api/proxy/health          # Health check do sistema
GET    /api/proxy/stats           # EstatÃ­sticas do proxy

GET    /api/conversations/:id     # HistÃ³rico da conversa
DELETE /api/conversations/:id     # Remove conversa

POST   /api/knowledge/ingest      # Ingere documentos (RAG)
GET    /api/knowledge/documents   # Lista documentos
DELETE /api/knowledge/documents/:doc # Remove documento

POST   /api/training/jobs         # Cria job de treinamento
GET    /api/training/jobs         # Lista jobs
GET    /api/training/jobs/:id     # Status do job

GET    /api/cache/stats           # EstatÃ­sticas do cache
DELETE /api/cache                 # Limpa cache
```

### WebSocket API

```javascript
// Conecta ao canal
channel = socket.channel("proxy:session_123", {user_id: "user_456"})

// Envia query
channel.push("query", {query: "Como funciona IA?"})

// Recebe resposta
channel.on("response", payload => {
  console.log(payload.response, payload.metadata)
})

// Streaming em tempo real
channel.push("stream_query", {query: "Explique machine learning"})
channel.on("stream_token", token => process(token))
channel.on("stream_complete", () => finish())
```

## ğŸ”„ Fluxo de Processamento

### 1. Recebimento da Query
```elixir
# Cliente â†’ ProxyController â†’ ConversationServer
ConversationServer.process_query(pid, "Como criar um chatbot?")
```

### 2. Pipeline de DecisÃ£o
```elixir
def process_query_pipeline(query, state, opts) do
  context = build_context(state.messages)

  # Etapa 1: Verifica cache
  case Cache.get(query, context) do
    {:ok, cached_response} ->
      {:ok, cached_response, %{source: "cache"}}

    {:error, :not_found} ->
      # Etapa 2: RAG + LLM
      case process_with_rag_and_llm(query, context) do
        {:ok, response, metadata} ->
          Cache.put(query, context, response)
          {:ok, response, metadata}

        {:error, :low_confidence} ->
          # Etapa 3: Fallback API B
          process_with_api_b(query, context)
      end
  end
end
```

### 3. GeraÃ§Ã£o com RAG
```elixir
def process_with_rag_and_llm(query, context) do
  # Busca contexto semÃ¢ntico
  {:ok, rag_context} = RAG.retrieve(query, limit: 5)

  # Gera resposta com LLM
  {:ok, response, confidence} = LLM.generate_response(
    query, context, rag_context, model: "gpt-4"
  )

  # Valida com guardrails
  case Guardrails.validate_response(response, query) do
    {:ok, validated} when confidence >= 0.7 ->
      {:ok, validated, %{source: "llm", confidence_score: confidence}}
    _ ->
      {:error, :low_confidence}
  end
end
```

## ğŸ¤– ConfiguraÃ§Ã£o de Modelos

### OpenAI
```elixir
# config/config.exs
config :neural_bridge, :openai_api_key, System.get_env("OPENAI_API_KEY")

# Uso com modelo especÃ­fico
LLM.generate_response(query, context, rag_context,
  provider: :openai,
  model: "gpt-4-turbo",
  temperature: 0.7
)
```

### Ollama
```elixir
# Uso com modelo especÃ­fico do Ollama
LLM.generate_response(query, context, rag_context,
  provider: :ollama,
  model: "llama2:13b",
  temperature: 0.5
)

# Embeddings com Ollama
LLM.generate_embedding(text,
  provider: :ollama,
  model: "nomic-embed-text"
)
```

## ğŸ“ˆ Jobs em Background

### Workers Implementados

1. **EmbedJob** - Gera embeddings para chunks de conhecimento
2. **TrainJob** - Fine-tuning e distillation de modelos
3. **CacheCleanupWorker** - Limpeza automÃ¡tica de cache
4. **TrainingDatasetWorker** - AnÃ¡lise diÃ¡ria e trigger de treinamento

```elixir
# Agenda job de embedding
NeuralBridge.Workers.EmbedJob.enqueue_chunk_embedding(chunk_id)

# Cria job de fine-tuning
NeuralBridge.Workers.TrainJob.create_training_job("fine_tune", %{
  dataset_limit: 1000,
  learning_rate: 0.0001,
  epochs: 3
})
```

## ğŸ›¡ï¸ Guardrails e SeguranÃ§a

### ValidaÃ§Ãµes Implementadas
- **DetecÃ§Ã£o de toxicidade** - Patterns para conteÃºdo prejudicial
- **Filtragem PII** - Remove informaÃ§Ãµes pessoais (CPF, email, etc)
- **ValidaÃ§Ã£o de qualidade** - Comprimento, repetiÃ§Ã£o, relevÃ¢ncia
- **ValidaÃ§Ã£o JSON Schema** - Para responses estruturadas

```elixir
# Exemplo de uso
case Guardrails.validate_response(response, query) do
  {:ok, clean_response} ->
    # Response aprovada
  {:error, :content_safety_violation} ->
    # ConteÃºdo bloqueado
  {:error, :low_relevance} ->
    # Resposta nÃ£o relevante
end
```

## ğŸ“Š MÃ©tricas e Observabilidade

### MÃ©tricas Telemetry
- `neural_bridge.queries.total` - Total de queries processadas
- `neural_bridge.cache.hit_rate` - Taxa de acerto do cache
- `neural_bridge.api_b.fallbacks.total` - Fallbacks para API B
- `neural_bridge.llm.confidence_score` - Scores de confianÃ§a
- `neural_bridge.training.jobs.total` - Jobs de treinamento

### Dashboards PromEx
- Dashboard de aplicaÃ§Ã£o (Beam/Phoenix)
- Dashboard customizado para mÃ©tricas do proxy LLM
- Monitoramento de Oban jobs
- EstatÃ­sticas de banco (Ecto)

## ğŸš€ Deploy e ConfiguraÃ§Ã£o

### VariÃ¡veis de Ambiente
```bash
# LLM Providers
export OPENAI_API_KEY="sk-..."
export OLLAMA_URL="http://localhost:11434"

# API B Fallback
export API_B_ENDPOINT="https://api.external.com"
export API_B_KEY="..."

# Database
export DATABASE_URL="postgresql://user:pass@localhost/neural_bridge_prod"
```

### Docker Compose (Exemplo)
```yaml
version: '3.8'
services:
  neural_bridge:
    build: .
    ports:
      - "4000:4000"
    environment:
      - DATABASE_URL=postgresql://postgres:password@db/neural_bridge
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    depends_on:
      - db

  db:
    image: pgvector/pgvector:pg15
    environment:
      - POSTGRES_DB=neural_bridge
      - POSTGRES_PASSWORD=password
    volumes:
      - postgres_data:/var/lib/postgresql/data
```

## ğŸ”§ Comandos Ãšteis

```bash
# Setup inicial
mix deps.get
mix ecto.create
mix ecto.migrate

# Desenvolvimento
mix phx.server
iex -S mix phx.server

# Jobs em background
iex> Oban.start_job(NeuralBridge.Workers.CacheCleanupWorker.new(%{}))

# Cache management
iex> NeuralBridge.Cache.clear_all()
iex> NeuralBridge.Cache.stats()

# RAG ingestion
iex> NeuralBridge.RAG.ingest_document(content, "doc1.pdf")
```

## ğŸ¯ Casos de Uso

### 1. Chatbot Inteligente
```javascript
// Frontend conecta via WebSocket
const response = await fetch('/api/proxy/query', {
  method: 'POST',
  headers: {'Content-Type': 'application/json'},
  body: JSON.stringify({
    query: "Como implementar autenticaÃ§Ã£o JWT?",
    session_id: "user_session_123",
    user_id: "user_456"
  })
})
```

### 2. Sistema de Suporte
- Cache automÃ¡tico de perguntas frequentes
- RAG com base de conhecimento da empresa
- Fallback para agentes humanos (API B)
- Treinamento contÃ­nuo com feedback

### 3. Assistente de CÃ³digo
- AnÃ¡lise de repositÃ³rios via RAG
- SugestÃµes de cÃ³digo com contexto
- RevisÃ£o automÃ¡tica de pull requests
- DocumentaÃ§Ã£o automÃ¡tica

## ğŸ”® PrÃ³ximos Passos

1. **pgvector Integration** - Habilitar busca semÃ¢ntica real
2. **FunÃ§Ã£o Calling** - Suporte a tools/functions do OpenAI
3. **Multi-tenant** - Isolamento por organizaÃ§Ã£o
4. **Rate Limiting** - Controle de taxa por usuÃ¡rio
5. **A/B Testing** - Testes entre diferentes modelos
6. **Fine-tuning Pipeline** - AutomaÃ§Ã£o completa de treinamento

---

## ğŸ“ Suporte

Para dÃºvidas tÃ©cnicas ou contribuiÃ§Ãµes, consulte:
- Logs: `mix phx.server` ou `/dev/dashboard`
- MÃ©tricas: PromEx dashboards
- Jobs: Oban Web UI
- CÃ³digo: RepositÃ³rio no GitHub

**VersÃ£o:** 1.0.0
**Ãšltima atualizaÃ§Ã£o:** 2024
**Maintainer:** NeuralBridge Team